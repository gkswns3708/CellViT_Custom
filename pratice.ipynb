{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd098cca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/CellViT_Custom/Checkpoints/CellViT/CellViT-SAM-H-x40.pth  - model_path\n",
      "Loading checkpoint: _IncompatibleKeys(missing_keys=['pos_embed', 'patch_embed.proj.weight', 'patch_embed.proj.bias', 'blocks.0.norm1.weight', 'blocks.0.norm1.bias', 'blocks.0.attn.rel_pos_h', 'blocks.0.attn.rel_pos_w', 'blocks.0.attn.qkv.weight', 'blocks.0.attn.qkv.bias', 'blocks.0.attn.proj.weight', 'blocks.0.attn.proj.bias', 'blocks.0.norm2.weight', 'blocks.0.norm2.bias', 'blocks.0.mlp.lin1.weight', 'blocks.0.mlp.lin1.bias', 'blocks.0.mlp.lin2.weight', 'blocks.0.mlp.lin2.bias', 'blocks.1.norm1.weight', 'blocks.1.norm1.bias', 'blocks.1.attn.rel_pos_h', 'blocks.1.attn.rel_pos_w', 'blocks.1.attn.qkv.weight', 'blocks.1.attn.qkv.bias', 'blocks.1.attn.proj.weight', 'blocks.1.attn.proj.bias', 'blocks.1.norm2.weight', 'blocks.1.norm2.bias', 'blocks.1.mlp.lin1.weight', 'blocks.1.mlp.lin1.bias', 'blocks.1.mlp.lin2.weight', 'blocks.1.mlp.lin2.bias', 'blocks.2.norm1.weight', 'blocks.2.norm1.bias', 'blocks.2.attn.rel_pos_h', 'blocks.2.attn.rel_pos_w', 'blocks.2.attn.qkv.weight', 'blocks.2.attn.qkv.bias', 'blocks.2.attn.proj.weight', 'blocks.2.attn.proj.bias', 'blocks.2.norm2.weight', 'blocks.2.norm2.bias', 'blocks.2.mlp.lin1.weight', 'blocks.2.mlp.lin1.bias', 'blocks.2.mlp.lin2.weight', 'blocks.2.mlp.lin2.bias', 'blocks.3.norm1.weight', 'blocks.3.norm1.bias', 'blocks.3.attn.rel_pos_h', 'blocks.3.attn.rel_pos_w', 'blocks.3.attn.qkv.weight', 'blocks.3.attn.qkv.bias', 'blocks.3.attn.proj.weight', 'blocks.3.attn.proj.bias', 'blocks.3.norm2.weight', 'blocks.3.norm2.bias', 'blocks.3.mlp.lin1.weight', 'blocks.3.mlp.lin1.bias', 'blocks.3.mlp.lin2.weight', 'blocks.3.mlp.lin2.bias', 'blocks.4.norm1.weight', 'blocks.4.norm1.bias', 'blocks.4.attn.rel_pos_h', 'blocks.4.attn.rel_pos_w', 'blocks.4.attn.qkv.weight', 'blocks.4.attn.qkv.bias', 'blocks.4.attn.proj.weight', 'blocks.4.attn.proj.bias', 'blocks.4.norm2.weight', 'blocks.4.norm2.bias', 'blocks.4.mlp.lin1.weight', 'blocks.4.mlp.lin1.bias', 'blocks.4.mlp.lin2.weight', 'blocks.4.mlp.lin2.bias', 'blocks.5.norm1.weight', 'blocks.5.norm1.bias', 'blocks.5.attn.rel_pos_h', 'blocks.5.attn.rel_pos_w', 'blocks.5.attn.qkv.weight', 'blocks.5.attn.qkv.bias', 'blocks.5.attn.proj.weight', 'blocks.5.attn.proj.bias', 'blocks.5.norm2.weight', 'blocks.5.norm2.bias', 'blocks.5.mlp.lin1.weight', 'blocks.5.mlp.lin1.bias', 'blocks.5.mlp.lin2.weight', 'blocks.5.mlp.lin2.bias', 'blocks.6.norm1.weight', 'blocks.6.norm1.bias', 'blocks.6.attn.rel_pos_h', 'blocks.6.attn.rel_pos_w', 'blocks.6.attn.qkv.weight', 'blocks.6.attn.qkv.bias', 'blocks.6.attn.proj.weight', 'blocks.6.attn.proj.bias', 'blocks.6.norm2.weight', 'blocks.6.norm2.bias', 'blocks.6.mlp.lin1.weight', 'blocks.6.mlp.lin1.bias', 'blocks.6.mlp.lin2.weight', 'blocks.6.mlp.lin2.bias', 'blocks.7.norm1.weight', 'blocks.7.norm1.bias', 'blocks.7.attn.rel_pos_h', 'blocks.7.attn.rel_pos_w', 'blocks.7.attn.qkv.weight', 'blocks.7.attn.qkv.bias', 'blocks.7.attn.proj.weight', 'blocks.7.attn.proj.bias', 'blocks.7.norm2.weight', 'blocks.7.norm2.bias', 'blocks.7.mlp.lin1.weight', 'blocks.7.mlp.lin1.bias', 'blocks.7.mlp.lin2.weight', 'blocks.7.mlp.lin2.bias', 'blocks.8.norm1.weight', 'blocks.8.norm1.bias', 'blocks.8.attn.rel_pos_h', 'blocks.8.attn.rel_pos_w', 'blocks.8.attn.qkv.weight', 'blocks.8.attn.qkv.bias', 'blocks.8.attn.proj.weight', 'blocks.8.attn.proj.bias', 'blocks.8.norm2.weight', 'blocks.8.norm2.bias', 'blocks.8.mlp.lin1.weight', 'blocks.8.mlp.lin1.bias', 'blocks.8.mlp.lin2.weight', 'blocks.8.mlp.lin2.bias', 'blocks.9.norm1.weight', 'blocks.9.norm1.bias', 'blocks.9.attn.rel_pos_h', 'blocks.9.attn.rel_pos_w', 'blocks.9.attn.qkv.weight', 'blocks.9.attn.qkv.bias', 'blocks.9.attn.proj.weight', 'blocks.9.attn.proj.bias', 'blocks.9.norm2.weight', 'blocks.9.norm2.bias', 'blocks.9.mlp.lin1.weight', 'blocks.9.mlp.lin1.bias', 'blocks.9.mlp.lin2.weight', 'blocks.9.mlp.lin2.bias', 'blocks.10.norm1.weight', 'blocks.10.norm1.bias', 'blocks.10.attn.rel_pos_h', 'blocks.10.attn.rel_pos_w', 'blocks.10.attn.qkv.weight', 'blocks.10.attn.qkv.bias', 'blocks.10.attn.proj.weight', 'blocks.10.attn.proj.bias', 'blocks.10.norm2.weight', 'blocks.10.norm2.bias', 'blocks.10.mlp.lin1.weight', 'blocks.10.mlp.lin1.bias', 'blocks.10.mlp.lin2.weight', 'blocks.10.mlp.lin2.bias', 'blocks.11.norm1.weight', 'blocks.11.norm1.bias', 'blocks.11.attn.rel_pos_h', 'blocks.11.attn.rel_pos_w', 'blocks.11.attn.qkv.weight', 'blocks.11.attn.qkv.bias', 'blocks.11.attn.proj.weight', 'blocks.11.attn.proj.bias', 'blocks.11.norm2.weight', 'blocks.11.norm2.bias', 'blocks.11.mlp.lin1.weight', 'blocks.11.mlp.lin1.bias', 'blocks.11.mlp.lin2.weight', 'blocks.11.mlp.lin2.bias', 'neck.0.weight', 'neck.1.weight', 'neck.1.bias', 'neck.2.weight', 'neck.3.weight', 'neck.3.bias'], unexpected_keys=['arch', 'epoch', 'model_state_dict', 'config'])\n"
     ]
    }
   ],
   "source": [
    "from Model.CellViTSAM import *\n",
    "\n",
    "\n",
    "pretrained_model = CellViTSAM(\n",
    "    model_path=Path(\"/workspace/CellViT_Custom/Checkpoints/CellViT/CellViT-SAM-H-x40.pth\"),  # 필수\n",
    "    num_nuclei_classes=6,                               # 필수 (배경 포함)\n",
    "    num_tissue_classes=19,                              # 필수 (조직 분류 안 하면 0)\n",
    "    vit_structure=\"SAM-B\",                              # 필수: \"SAM-B\" | \"SAM-L\" | \"SAM-H\"\n",
    "    drop_rate=0.0,                                      # 옵션\n",
    "    regression_loss=False                               # 옵션\n",
    ")\n",
    "\n",
    "pretrained_model.load_pretrained_encoder(Path(\"/workspace/CellViT_Custom/Checkpoints/CellViT/CellViT-SAM-H-x40.pth\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "70c25104",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model = CellViTSAM(\n",
    "    model_path=Path(\"/workspace/CellViT_Custom/Checkpoints/CellViT/CellViT-SAM-H-x40.pth\"),  # 필수\n",
    "    num_nuclei_classes=6,                               # 필수 (배경 포함)\n",
    "    num_tissue_classes=19,                              # 필수 (조직 분류 안 하면 0)\n",
    "    vit_structure=\"SAM-B\",                              # 필수: \"SAM-B\" | \"SAM-L\" | \"SAM-H\"\n",
    "    drop_rate=0.0,                                      # 옵션\n",
    "    regression_loss=False                               # 옵션\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "de6dcc87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Different layers: ['patch_embed.proj.weight', 'patch_embed.proj.bias', 'blocks.0.attn.qkv.weight', 'blocks.0.attn.qkv.bias', 'blocks.0.attn.proj.weight', 'blocks.0.attn.proj.bias', 'blocks.0.mlp.lin1.weight', 'blocks.0.mlp.lin1.bias', 'blocks.0.mlp.lin2.weight', 'blocks.0.mlp.lin2.bias', 'blocks.1.attn.qkv.weight', 'blocks.1.attn.qkv.bias', 'blocks.1.attn.proj.weight', 'blocks.1.attn.proj.bias', 'blocks.1.mlp.lin1.weight', 'blocks.1.mlp.lin1.bias', 'blocks.1.mlp.lin2.weight', 'blocks.1.mlp.lin2.bias', 'blocks.2.attn.qkv.weight', 'blocks.2.attn.qkv.bias', 'blocks.2.attn.proj.weight', 'blocks.2.attn.proj.bias', 'blocks.2.mlp.lin1.weight', 'blocks.2.mlp.lin1.bias', 'blocks.2.mlp.lin2.weight', 'blocks.2.mlp.lin2.bias', 'blocks.3.attn.qkv.weight', 'blocks.3.attn.qkv.bias', 'blocks.3.attn.proj.weight', 'blocks.3.attn.proj.bias', 'blocks.3.mlp.lin1.weight', 'blocks.3.mlp.lin1.bias', 'blocks.3.mlp.lin2.weight', 'blocks.3.mlp.lin2.bias', 'blocks.4.attn.qkv.weight', 'blocks.4.attn.qkv.bias', 'blocks.4.attn.proj.weight', 'blocks.4.attn.proj.bias', 'blocks.4.mlp.lin1.weight', 'blocks.4.mlp.lin1.bias', 'blocks.4.mlp.lin2.weight', 'blocks.4.mlp.lin2.bias', 'blocks.5.attn.qkv.weight', 'blocks.5.attn.qkv.bias', 'blocks.5.attn.proj.weight', 'blocks.5.attn.proj.bias', 'blocks.5.mlp.lin1.weight', 'blocks.5.mlp.lin1.bias', 'blocks.5.mlp.lin2.weight', 'blocks.5.mlp.lin2.bias', 'blocks.6.attn.qkv.weight', 'blocks.6.attn.qkv.bias', 'blocks.6.attn.proj.weight', 'blocks.6.attn.proj.bias', 'blocks.6.mlp.lin1.weight', 'blocks.6.mlp.lin1.bias', 'blocks.6.mlp.lin2.weight', 'blocks.6.mlp.lin2.bias', 'blocks.7.attn.qkv.weight', 'blocks.7.attn.qkv.bias', 'blocks.7.attn.proj.weight', 'blocks.7.attn.proj.bias', 'blocks.7.mlp.lin1.weight', 'blocks.7.mlp.lin1.bias', 'blocks.7.mlp.lin2.weight', 'blocks.7.mlp.lin2.bias', 'blocks.8.attn.qkv.weight', 'blocks.8.attn.qkv.bias', 'blocks.8.attn.proj.weight', 'blocks.8.attn.proj.bias', 'blocks.8.mlp.lin1.weight', 'blocks.8.mlp.lin1.bias', 'blocks.8.mlp.lin2.weight', 'blocks.8.mlp.lin2.bias', 'blocks.9.attn.qkv.weight', 'blocks.9.attn.qkv.bias', 'blocks.9.attn.proj.weight', 'blocks.9.attn.proj.bias', 'blocks.9.mlp.lin1.weight', 'blocks.9.mlp.lin1.bias', 'blocks.9.mlp.lin2.weight', 'blocks.9.mlp.lin2.bias', 'blocks.10.attn.qkv.weight', 'blocks.10.attn.qkv.bias', 'blocks.10.attn.proj.weight', 'blocks.10.attn.proj.bias', 'blocks.10.mlp.lin1.weight', 'blocks.10.mlp.lin1.bias', 'blocks.10.mlp.lin2.weight', 'blocks.10.mlp.lin2.bias', 'blocks.11.attn.qkv.weight', 'blocks.11.attn.qkv.bias', 'blocks.11.attn.proj.weight', 'blocks.11.attn.proj.bias', 'blocks.11.mlp.lin1.weight', 'blocks.11.mlp.lin1.bias', 'blocks.11.mlp.lin2.weight', 'blocks.11.mlp.lin2.bias', 'neck.0.weight', 'neck.2.weight']\n"
     ]
    }
   ],
   "source": [
    "# 두 모델의 encoder state_dict를 가져옴\n",
    "new_state = new_model.encoder.state_dict()\n",
    "pretrained_state = pretrained_model.encoder.state_dict()\n",
    "\n",
    "# 서로 다른 weight를 저장할 dict\n",
    "diffs = {}\n",
    "\n",
    "for k in new_state:\n",
    "    if k in pretrained_state:\n",
    "        if not torch.equal(new_state[k], pretrained_state[k]):\n",
    "            diffs[k] = {\n",
    "                \"new\": new_state[k].clone(),\n",
    "                \"pretrained\": pretrained_state[k].clone()\n",
    "            }\n",
    "    else:\n",
    "        print(f\"{k} is not in pretrained model.\")\n",
    "\n",
    "for k in pretrained_state:\n",
    "    if k not in new_state:\n",
    "        print(f\"{k} is not in new model.\")\n",
    "\n",
    "print(f\"Different layers: {list(diffs.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "404dd138",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/cellvit_env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"RationAI/PanNuke\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "317de972",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ds['fold1'][3]['instances'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6f76c1da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'image': <PIL.PngImagePlugin.PngImageFile image mode=RGB size=256x256>,\n",
       " 'instances': [<PIL.PngImagePlugin.PngImageFile image mode=1 size=256x256>,\n",
       "  <PIL.PngImagePlugin.PngImageFile image mode=1 size=256x256>,\n",
       "  <PIL.PngImagePlugin.PngImageFile image mode=1 size=256x256>,\n",
       "  <PIL.PngImagePlugin.PngImageFile image mode=1 size=256x256>,\n",
       "  <PIL.PngImagePlugin.PngImageFile image mode=1 size=256x256>,\n",
       "  <PIL.PngImagePlugin.PngImageFile image mode=1 size=256x256>,\n",
       "  <PIL.PngImagePlugin.PngImageFile image mode=1 size=256x256>,\n",
       "  <PIL.PngImagePlugin.PngImageFile image mode=1 size=256x256>,\n",
       "  <PIL.PngImagePlugin.PngImageFile image mode=1 size=256x256>,\n",
       "  <PIL.PngImagePlugin.PngImageFile image mode=1 size=256x256>],\n",
       " 'categories': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " 'tissue': 3}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds['fold1'][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e5f9409f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "data = np.load('/workspace/CellViT_Custom/Dataset/CoNSeP/Preprocessed/Train/labels/train_1_r0_c0.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b56a2fd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/432 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 432/432 [00:00<00:00, 1520.23it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "defaultdict(int, {0: 432, 2: 237, 5: 286, 6: 89, 3: 47, 1: 102, 4: 134, 7: 17})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from glob import glob\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "label_counts = defaultdict(int)\n",
    "for path in tqdm(glob('/workspace/CellViT_Custom/Dataset/CoNSeP/Preprocessed/Train/labels/*.npz')):\n",
    "    data = np.load(path)\n",
    "    for label in np.unique(data['type_map']):\n",
    "        label_counts[label] += 1\n",
    "\n",
    "label_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e05f2be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f362fd43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: 3.11.9 | packaged by conda-forge | (main, Apr 19 2024, 18:36:13) [GCC 12.3.0]\n",
      "PyTorch: 2.4.1+cu118\n",
      "CUDA available: True\n"
     ]
    }
   ],
   "source": [
    "# --- 셀 1: 공통 임포트 & 유틸 ---\n",
    "import os, sys, re, json, math, textwrap\n",
    "from pathlib import Path\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "import torch\n",
    "\n",
    "# 워크스페이스 루트가 노트북 현재 경로 기준 어디인지에 따라 수정하세요.\n",
    "# 예시: /workspace/CellViT_Custom 가 repo 루트라면:\n",
    "repo_root = Path.cwd()  # 필요시 Path(\"/workspace/CellViT_Custom\") 등으로 고정\n",
    "if str(repo_root) not in sys.path:\n",
    "    sys.path.insert(0, str(repo_root))\n",
    "\n",
    "print(\"Python:\", sys.version)\n",
    "print(\"PyTorch:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "\n",
    "def group_by_prefix(keys, depth=2, topk=30):\n",
    "    \"\"\"\n",
    "    'a.b.c', 'a.b.d' 같은 키들을 depth=2 기준으로 그룹핑해 상위 분포를 보여줍니다.\n",
    "    \"\"\"\n",
    "    buckets = Counter(\".\".join(k.split(\".\")[:depth]) for k in keys)\n",
    "    total = sum(buckets.values())\n",
    "    for k, v in buckets.most_common(topk):\n",
    "        print(f\"{k:<40} {v:>6}  ({v/total*100:5.1f}%)\")\n",
    "    print(f\"Total keys: {total}\")\n",
    "\n",
    "def sample_list(lst, n=20):\n",
    "    lst = list(lst)\n",
    "    n = min(n, len(lst))\n",
    "    return lst[:n]\n",
    "\n",
    "def print_key_samples(title, keys, n=30):\n",
    "    keys = list(keys)\n",
    "    keys.sort()\n",
    "    print(f\"\\n[{title}] count={len(keys)}\")\n",
    "    for k in keys[:n]:\n",
    "        print(\"  \", k)\n",
    "    if len(keys) > n:\n",
    "        print(\"  ...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5161041b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model param count: 46.74M\n",
      "Model state_dict keys: 437\n",
      "모델 키 상위 분포:\n",
      "encoder.blocks                              144  ( 33.0%)\n",
      "hv_map_decoder.decoder3_upsampler            23  (  5.3%)\n",
      "nuclei_type_maps_decoder.decoder3_upsampler     23  (  5.3%)\n",
      "nuclei_binary_map_decoder.decoder3_upsampler     23  (  5.3%)\n",
      "hv_map_decoder.decoder1_upsampler            16  (  3.7%)\n",
      "nuclei_type_maps_decoder.decoder1_upsampler     16  (  3.7%)\n",
      "hv_map_decoder.decoder2_upsampler            16  (  3.7%)\n",
      "nuclei_type_maps_decoder.decoder0_header     16  (  3.7%)\n",
      "nuclei_binary_map_decoder.decoder1_upsampler     16  (  3.7%)\n",
      "nuclei_binary_map_decoder.decoder0_header     16  (  3.7%)\n",
      "hv_map_decoder.decoder0_header               16  (  3.7%)\n",
      "nuclei_type_maps_decoder.decoder2_upsampler     16  (  3.7%)\n",
      "nuclei_binary_map_decoder.decoder2_upsampler     16  (  3.7%)\n",
      "decoder2.0                                    9  (  2.1%)\n",
      "decoder1.0                                    9  (  2.1%)\n",
      "decoder1.2                                    9  (  2.1%)\n",
      "decoder3.0                                    9  (  2.1%)\n",
      "decoder2.1                                    9  (  2.1%)\n",
      "decoder1.1                                    9  (  2.1%)\n",
      "decoder0.1                                    7  (  1.6%)\n",
      "decoder0.0                                    7  (  1.6%)\n",
      "encoder.patch_embed                           2  (  0.5%)\n",
      "nuclei_type_maps_decoder.bottleneck_upsampler      2  (  0.5%)\n",
      "hv_map_decoder.bottleneck_upsampler           2  (  0.5%)\n",
      "nuclei_binary_map_decoder.bottleneck_upsampler      2  (  0.5%)\n",
      "encoder.norm                                  2  (  0.5%)\n",
      "encoder.cls_token                             1  (  0.2%)\n",
      "encoder.pos_embed                             1  (  0.2%)\n",
      "Total keys: 437\n"
     ]
    }
   ],
   "source": [
    "# --- 셀 2: 모델 임포트 & 빌드 ---\n",
    "from Model.CellViT_ViT256_Custom import CellViTCustom  # 훈련 스크립트와 동일\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# train_custom_ViT.py 의 기본값/CLI 인자에 맞춰 구성\n",
    "cfg = dict(\n",
    "    num_nuclei_classes=6,        # PanNuke(0:bg+5) = 6\n",
    "    num_tissue_classes=0,        # PanNuke tissue label 미사용\n",
    "    img_size=256,\n",
    "    patch_size=16,\n",
    "    embed_dim=384,\n",
    "    depth=12,\n",
    "    num_heads=6,\n",
    "    mlp_ratio=4.0,\n",
    ")\n",
    "\n",
    "model = CellViTCustom(**cfg).to(device)\n",
    "model.eval()\n",
    "\n",
    "model_keys = set(model.state_dict().keys())\n",
    "print(f\"Model param count: {sum(p.numel() for p in model.parameters())/1e6:.2f}M\")\n",
    "print(f\"Model state_dict keys: {len(model_keys)}\")\n",
    "print(\"모델 키 상위 분포:\")\n",
    "group_by_prefix(model_keys, depth=2, topk=30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f3796723",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CKPT keys: 439\n",
      "체크포인트 키 상위 분포:\n",
      "encoder.blocks                              144  ( 32.8%)\n",
      "hv_map_decoder.decoder3_upsampler            23  (  5.2%)\n",
      "nuclei_type_maps_decoder.decoder3_upsampler     23  (  5.2%)\n",
      "nuclei_binary_map_decoder.decoder3_upsampler     23  (  5.2%)\n",
      "hv_map_decoder.decoder1_upsampler            16  (  3.6%)\n",
      "nuclei_type_maps_decoder.decoder1_upsampler     16  (  3.6%)\n",
      "hv_map_decoder.decoder2_upsampler            16  (  3.6%)\n",
      "nuclei_type_maps_decoder.decoder0_header     16  (  3.6%)\n",
      "nuclei_binary_map_decoder.decoder1_upsampler     16  (  3.6%)\n",
      "nuclei_binary_map_decoder.decoder0_header     16  (  3.6%)\n",
      "hv_map_decoder.decoder0_header               16  (  3.6%)\n",
      "nuclei_type_maps_decoder.decoder2_upsampler     16  (  3.6%)\n",
      "nuclei_binary_map_decoder.decoder2_upsampler     16  (  3.6%)\n",
      "decoder2.0                                    9  (  2.1%)\n",
      "decoder1.0                                    9  (  2.1%)\n",
      "decoder1.2                                    9  (  2.1%)\n",
      "decoder3.0                                    9  (  2.1%)\n",
      "decoder2.1                                    9  (  2.1%)\n",
      "decoder1.1                                    9  (  2.1%)\n",
      "decoder0.1                                    7  (  1.6%)\n",
      "decoder0.0                                    7  (  1.6%)\n",
      "encoder.patch_embed                           2  (  0.5%)\n",
      "nuclei_type_maps_decoder.bottleneck_upsampler      2  (  0.5%)\n",
      "hv_map_decoder.bottleneck_upsampler           2  (  0.5%)\n",
      "nuclei_binary_map_decoder.bottleneck_upsampler      2  (  0.5%)\n",
      "encoder.head                                  2  (  0.5%)\n",
      "encoder.norm                                  2  (  0.5%)\n",
      "encoder.cls_token                             1  (  0.2%)\n",
      "encoder.pos_embed                             1  (  0.2%)\n",
      "Total keys: 439\n",
      "\n",
      "[CKPT에만 있고 모델에 없는 키(예시)] count=2\n",
      "   encoder.head.bias\n",
      "   encoder.head.weight\n",
      "\n",
      "[모델에만 있고 CKPT엔 없는 키(예시)] count=0\n",
      "\n",
      "[양쪽에 모두 있는 키(예시)] count=437\n",
      "   decoder0.0.block.0.bias\n",
      "   decoder0.0.block.0.weight\n",
      "   decoder0.0.block.1.bias\n",
      "   decoder0.0.block.1.num_batches_tracked\n",
      "   decoder0.0.block.1.running_mean\n",
      "   decoder0.0.block.1.running_var\n",
      "   decoder0.0.block.1.weight\n",
      "   decoder0.1.block.0.bias\n",
      "   decoder0.1.block.0.weight\n",
      "   decoder0.1.block.1.bias\n",
      "   decoder0.1.block.1.num_batches_tracked\n",
      "   decoder0.1.block.1.running_mean\n",
      "   decoder0.1.block.1.running_var\n",
      "   decoder0.1.block.1.weight\n",
      "   decoder1.0.block.0.bias\n",
      "   decoder1.0.block.0.weight\n",
      "   decoder1.0.block.1.bias\n",
      "   decoder1.0.block.1.weight\n",
      "   decoder1.0.block.2.bias\n",
      "   decoder1.0.block.2.num_batches_tracked\n",
      "   decoder1.0.block.2.running_mean\n",
      "   decoder1.0.block.2.running_var\n",
      "   decoder1.0.block.2.weight\n",
      "   decoder1.1.block.0.bias\n",
      "   decoder1.1.block.0.weight\n",
      "   decoder1.1.block.1.bias\n",
      "   decoder1.1.block.1.weight\n",
      "   decoder1.1.block.2.bias\n",
      "   decoder1.1.block.2.num_batches_tracked\n",
      "   decoder1.1.block.2.running_mean\n",
      "   decoder1.1.block.2.running_var\n",
      "   decoder1.1.block.2.weight\n",
      "   decoder1.2.block.0.bias\n",
      "   decoder1.2.block.0.weight\n",
      "   decoder1.2.block.1.bias\n",
      "   decoder1.2.block.1.weight\n",
      "   decoder1.2.block.2.bias\n",
      "   decoder1.2.block.2.num_batches_tracked\n",
      "   decoder1.2.block.2.running_mean\n",
      "   decoder1.2.block.2.running_var\n",
      "  ...\n"
     ]
    }
   ],
   "source": [
    "# --- 셀 3: 체크포인트 로드 & 키 비교(1차) ---\n",
    "ckpt_path = Path(\"Checkpoints/CellViT/CellViT-256-x40.pth\")  # 질문에서 사용한 경로\n",
    "assert ckpt_path.exists(), f\"ckpt 경로가 없어요: {ckpt_path}\"\n",
    "\n",
    "raw = torch.load(str(ckpt_path), map_location=\"cpu\")\n",
    "sd = raw.get(\"model_state_dict\", raw)  # {'model': state_dict} 형식일 수도 있으니\n",
    "\n",
    "ckpt_keys = set(sd.keys())\n",
    "print(f\"CKPT keys: {len(ckpt_keys)}\")\n",
    "print(\"체크포인트 키 상위 분포:\")\n",
    "group_by_prefix(ckpt_keys, depth=2, topk=40)\n",
    "\n",
    "# 1차 단순 비교\n",
    "only_in_ckpt = ckpt_keys - model_keys\n",
    "only_in_model = model_keys - ckpt_keys\n",
    "in_both = ckpt_keys & model_keys\n",
    "\n",
    "print_key_samples(\"CKPT에만 있고 모델에 없는 키(예시)\", only_in_ckpt, n=40)\n",
    "print_key_samples(\"모델에만 있고 CKPT엔 없는 키(예시)\", only_in_model, n=40)\n",
    "print_key_samples(\"양쪽에 모두 있는 키(예시)\", in_both, n=40)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c74bbe3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
